% python3 test2_cartpole.py 
/Users/anupkaul/akaul_git/drl/simenvs/cross_entropy/test2_cartpole.py:117: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)
  obs_v = torch.FloatTensor([obs])
0: loss=0.691, reward_mean=25.1, reward_bound=29.0
1: loss=0.673, reward_mean=24.8, reward_bound=31.5
2: loss=0.663, reward_mean=32.1, reward_bound=37.5
3: loss=0.653, reward_mean=31.1, reward_bound=34.0
4: loss=0.640, reward_mean=40.5, reward_bound=50.0
5: loss=0.620, reward_mean=36.9, reward_bound=41.5
6: loss=0.611, reward_mean=46.6, reward_bound=48.5
7: loss=0.609, reward_mean=45.8, reward_bound=54.0
8: loss=0.582, reward_mean=45.5, reward_bound=48.5
9: loss=0.617, reward_mean=57.6, reward_bound=64.0
10: loss=0.582, reward_mean=65.7, reward_bound=76.5
11: loss=0.573, reward_mean=77.8, reward_bound=83.0
12: loss=0.583, reward_mean=66.0, reward_bound=72.0
13: loss=0.556, reward_mean=76.0, reward_bound=97.5
14: loss=0.548, reward_mean=69.1, reward_bound=76.5
15: loss=0.550, reward_mean=71.6, reward_bound=70.5
16: loss=0.541, reward_mean=62.1, reward_bound=72.5
17: loss=0.519, reward_mean=74.2, reward_bound=73.5
18: loss=0.518, reward_mean=83.9, reward_bound=94.5
19: loss=0.535, reward_mean=80.1, reward_bound=98.0
20: loss=0.512, reward_mean=85.2, reward_bound=91.5
21: loss=0.526, reward_mean=80.5, reward_bound=87.0
22: loss=0.501, reward_mean=113.1, reward_bound=125.0
23: loss=0.511, reward_mean=112.4, reward_bound=126.0
24: loss=0.506, reward_mean=93.6, reward_bound=110.0
25: loss=0.492, reward_mean=105.8, reward_bound=108.5
26: loss=0.486, reward_mean=97.4, reward_bound=112.0
27: loss=0.499, reward_mean=100.7, reward_bound=106.0
28: loss=0.470, reward_mean=105.0, reward_bound=111.0
29: loss=0.472, reward_mean=105.9, reward_bound=131.5
30: loss=0.457, reward_mean=105.2, reward_bound=115.5
31: loss=0.447, reward_mean=98.1, reward_bound=102.0
32: loss=0.475, reward_mean=114.5, reward_bound=141.0
33: loss=0.450, reward_mean=105.1, reward_bound=111.5
34: loss=0.471, reward_mean=113.2, reward_bound=129.5
35: loss=0.439, reward_mean=121.2, reward_bound=131.0
36: loss=0.441, reward_mean=133.5, reward_bound=145.5
37: loss=0.439, reward_mean=124.6, reward_bound=129.0
38: loss=0.437, reward_mean=136.9, reward_bound=164.0
39: loss=0.429, reward_mean=119.5, reward_bound=121.5
40: loss=0.425, reward_mean=129.3, reward_bound=145.0
41: loss=0.428, reward_mean=126.8, reward_bound=147.5
42: loss=0.441, reward_mean=139.4, reward_bound=146.5
43: loss=0.408, reward_mean=128.5, reward_bound=135.5
44: loss=0.425, reward_mean=124.7, reward_bound=150.5
45: loss=0.413, reward_mean=156.6, reward_bound=156.5
46: loss=0.433, reward_mean=151.0, reward_bound=169.0
47: loss=0.420, reward_mean=167.6, reward_bound=189.0
48: loss=0.422, reward_mean=168.1, reward_bound=195.0
49: loss=0.417, reward_mean=185.7, reward_bound=193.0
50: loss=0.421, reward_mean=217.3, reward_bound=235.0
RL Solved !!

51: loss=0.423, reward_mean=238.0, reward_bound=247.5
RL Solved !!

52: loss=0.431, reward_mean=231.1, reward_bound=215.5
RL Solved !!

53: loss=0.444, reward_mean=233.8, reward_bound=258.0
RL Solved !!

54: loss=0.438, reward_mean=294.2, reward_bound=342.5
RL Solved !!

55: loss=0.431, reward_mean=351.4, reward_bound=365.0
RL Solved !!

56: loss=0.424, reward_mean=418.2, reward_bound=520.0
RL Solved !!

57: loss=0.428, reward_mean=436.2, reward_bound=476.0
RL Solved !!

58: loss=0.428, reward_mean=481.0, reward_bound=477.5
RL Solved !!

59: loss=0.436, reward_mean=593.9, reward_bound=597.0
RL Solved !!

60: loss=0.436, reward_mean=455.8, reward_bound=472.5
RL Solved !!
